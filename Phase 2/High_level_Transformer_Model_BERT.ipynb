{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformer Based Enconding and Model\n","metadata":{}},{"cell_type":"code","source":"!pip install transformers==2.3.0","metadata":{"id":"SuY02mplBrDy","outputId":"7812630a-8a77-4cdf-b99b-892778ca3898","execution":{"iopub.status.busy":"2023-04-08T14:45:16.196930Z","iopub.execute_input":"2023-04-08T14:45:16.197245Z","iopub.status.idle":"2023-04-08T14:45:25.629940Z","shell.execute_reply.started":"2023-04-08T14:45:16.197177Z","shell.execute_reply":"2023-04-08T14:45:25.628979Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting transformers==2.3.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n\u001b[K     |████████████████████████████████| 450kB 4.2MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0) (0.1.83)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0) (1.10.29)\nCollecting sacremoses\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/78/fef8d089db5b97546fd6d1ff2e813b8544e85670bf3a8c378c9d0250b98d/sacremoses-0.0.53.tar.gz (880kB)\n\u001b[K     |████████████████████████████████| 880kB 32.8MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0) (2.22.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0) (1.17.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0) (4.39.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0) (2019.11.1)\nRequirement already satisfied: botocore<1.14.0,>=1.13.29 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0) (1.13.29)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0) (0.9.4)\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0) (0.2.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.3.0) (1.13.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.3.0) (7.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.3.0) (0.14.0)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0) (2.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0) (1.24.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0) (2019.9.11)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0) (3.0.4)\nRequirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.29->boto3->transformers==2.3.0) (2.8.0)\nRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.29->boto3->transformers==2.3.0) (0.15.2)\nBuilding wheels for collected packages: sacremoses\n  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-cp36-none-any.whl size=895254 sha256=3fc4a7282b5ca286abd4c8f0fc9e020a7329c09369115751c4d7ce756deacad9\n  Stored in directory: /root/.cache/pip/wheels/56/d5/b2/bc878b2bbddfbcc8fd62ca73c4fd842bd28c1fd3dbdf424c74\nSuccessfully built sacremoses\nInstalling collected packages: sacremoses, transformers\nSuccessfully installed sacremoses-0.0.53 transformers-2.3.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report","metadata":{"id":"aGa2jmNmCjXf","execution":{"iopub.status.busy":"2023-04-08T14:45:25.632030Z","iopub.execute_input":"2023-04-08T14:45:25.632327Z","iopub.status.idle":"2023-04-08T14:45:31.131481Z","shell.execute_reply.started":"2023-04-08T14:45:25.632280Z","shell.execute_reply":"2023-04-08T14:45:31.130611Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### BERT Embedding","metadata":{}},{"cell_type":"code","source":"# Loading the data\n\ntrain_df = pd.read_csv('/kaggle/input/fea-eng-toxiccomments/final_data.csv')\n\n\n# features = ['sentence_count', 'word_count', 'unique_word_count', \n#             'length', 'punctuation_count', 'upper_case_count', \n#             'stopword_count', '#_count', 'unique_word_count_percent', \n#             'Punctuation_percent', 'ip_count','link_count', \n#             'article_id_count', 'username_count', 'clean_comment']\n\ntarget_col = ['toxic', 'severe_toxic', 'obscene', 'threat',\n               'insult', 'identity_hate']\ntrain_df = train_df[['id', 'clean_comment', 'toxic', 'severe_toxic', 'obscene', 'threat',\n       'insult', 'identity_hate']]\ntrain_df = train_df.dropna()","metadata":{"id":"50uZZZzH_NDV","outputId":"f925ad52-96d5-47a1-b573-db6fa12012ba","execution":{"iopub.status.busy":"2023-04-08T15:05:32.789668Z","iopub.execute_input":"2023-04-08T15:05:32.789965Z","iopub.status.idle":"2023-04-08T15:05:34.464782Z","shell.execute_reply.started":"2023-04-08T15:05:32.789913Z","shell.execute_reply":"2023-04-08T15:05:34.463967Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nbert_model_name = 'bert-base-uncased'\n\ntokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\nMAX_LEN = 128\n\ndef tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n    tokenized_sentences = []\n\n    for sentence in tqdm(sentences):\n        tokenized_sentence = tokenizer.encode(\n                            sentence,                  # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = max_seq_len,  # Truncate all sentences.\n                    )\n        \n        tokenized_sentences.append(tokenized_sentence)\n\n    return tokenized_sentences\n\ndef create_attention_masks(tokenized_and_padded_sentences):\n    attention_masks = []\n\n    for sentence in tokenized_and_padded_sentences:\n        att_mask = [int(token_id > 0) for token_id in sentence]\n        attention_masks.append(att_mask)\n\n    return np.asarray(attention_masks)\n\ninput_ids = tokenize_sentences(train_df['clean_comment'], tokenizer, MAX_LEN)\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\nattention_masks = create_attention_masks(input_ids)","metadata":{"id":"g-8WnPfD_35q","outputId":"ef31e72c-9dc5-4e7d-875e-875758466538","execution":{"iopub.status.busy":"2023-04-08T15:05:34.466701Z","iopub.execute_input":"2023-04-08T15:05:34.466999Z","iopub.status.idle":"2023-04-08T15:08:57.819777Z","shell.execute_reply.started":"2023-04-08T15:05:34.466950Z","shell.execute_reply":"2023-04-08T15:08:57.818825Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=159508), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16ed64b60fee4e94b817558917a5dd0e"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Splitting train and test dataset","metadata":{}},{"cell_type":"code","source":"# splitting train and test dataset\n\nfrom sklearn.model_selection import train_test_split\n\nlabels =  train_df[target_col].values\n\ntrain_inputs, test_inputs, train_labels, test_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.1)\ntrain_masks, test_masks, train_labels2, test_labels2 = train_test_split(attention_masks, labels, random_state=0, test_size=0.1)\n\ntrain_size = len(train_inputs)\ntest_size = len(test_inputs)","metadata":{"id":"bMumeio9FFds","execution":{"iopub.status.busy":"2023-04-08T15:08:57.821174Z","iopub.execute_input":"2023-04-08T15:08:57.821485Z","iopub.status.idle":"2023-04-08T15:08:57.976970Z","shell.execute_reply.started":"2023-04-08T15:08:57.821425Z","shell.execute_reply":"2023-04-08T15:08:57.976195Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Creating TensorFlow dataset","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\nNR_EPOCHS = 1\n\ndef create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n    if train:\n        dataset = dataset.shuffle(buffer_size=buffer_size)\n    dataset = dataset.repeat(epochs)\n    dataset = dataset.batch(batch_size)\n    if train:\n        dataset = dataset.prefetch(1)\n    \n    return dataset\n\ntrain_dataset = create_dataset((train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\ntest_dataset = create_dataset((test_inputs, test_masks, test_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)","metadata":{"id":"H99t9YNDFcRq","execution":{"iopub.status.busy":"2023-04-08T15:08:57.978680Z","iopub.execute_input":"2023-04-08T15:08:57.979133Z","iopub.status.idle":"2023-04-08T15:08:58.251832Z","shell.execute_reply.started":"2023-04-08T15:08:57.978947Z","shell.execute_reply":"2023-04-08T15:08:58.250969Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#### BERT Modelling","metadata":{}},{"cell_type":"code","source":"from transformers import TFBertModel\nfrom tensorflow.keras.layers import Dense, Flatten\n\nclass BertClassifier(tf.keras.Model):    \n    def __init__(self, bert: TFBertModel, num_classes: int):\n        super().__init__()\n        self.bert = bert\n        self.classifier = Dense(num_classes, activation='sigmoid')\n        \n    @tf.function\n    # Loading pre-trained BERT model\n    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        outputs = self.bert(input_ids,\n                               attention_mask=attention_mask,\n                               token_type_ids=token_type_ids,\n                               position_ids=position_ids,\n                               head_mask=head_mask)\n        cls_output = outputs[1]\n        cls_output = self.classifier(cls_output)\n                \n        return cls_output\n\nmodel = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(target_col))","metadata":{"id":"3Zp6X7meF_T5","execution":{"iopub.status.busy":"2023-04-08T15:08:58.254554Z","iopub.execute_input":"2023-04-08T15:08:58.254850Z","iopub.status.idle":"2023-04-08T15:09:12.652731Z","shell.execute_reply.started":"2023-04-08T15:08:58.254800Z","shell.execute_reply":"2023-04-08T15:09:12.651737Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import time\nfrom transformers import create_optimizer\n\nsteps_per_epoch = train_size // BATCH_SIZE\ntest_steps = test_size // BATCH_SIZE\n\n# loss Function\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\n\n# Optimizer \nwarmup_steps = steps_per_epoch // 3\ntotal_steps = steps_per_epoch * NR_EPOCHS - warmup_steps\noptimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\n\n# Metrics\ntrain_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(target_col))]\ntest_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(target_col))]\n\n\n@tf.function\ndef train_step(model, token_ids, masks, labels):\n    labels = tf.dtypes.cast(labels, tf.float32)\n\n    with tf.GradientTape() as tape:\n        predictions = model(token_ids, attention_mask=masks)\n        loss = loss_object(labels, predictions)\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables), 1.0)\n\n    train_loss(loss)\n\n    for i, auc in enumerate(train_auc_metrics):\n        auc.update_state(labels[:,i], predictions[:,i])\n        \n@tf.function\ndef predict(model, token_ids, masks, labels):\n    labels = tf.dtypes.cast(labels, tf.float32)\n\n    predictions = model(token_ids, attention_mask=masks, training=False)\n    v_loss = loss_object(labels, predictions)\n\n    test_loss(v_loss)\n    for i, auc in enumerate(test_auc_metrics):\n        auc.update_state(labels[:,i], predictions[:,i])\n        \n    return predictions\n                                              \ndef train(model, train_dataset, test_dataset, train_steps_per_epoch, test_steps_per_epoch, epochs):\n    for epoch in range(epochs):\n        for i, (token_ids, masks, labels) in enumerate(tqdm(train_dataset, total=train_steps_per_epoch)):\n            train_step(model, token_ids, masks, labels)\n            if i % 1000 == 0:\n                print(f'\\nTrain Step: {i}, Loss: {train_loss.result()}')\n        \n        \n","metadata":{"id":"cJsVaX4qqoM0","outputId":"52dfd86b-a8f4-48f5-c481-a9db4f5c40fa","execution":{"iopub.status.busy":"2023-04-08T15:09:12.654285Z","iopub.execute_input":"2023-04-08T15:09:12.654575Z","iopub.status.idle":"2023-04-08T15:09:12.753194Z","shell.execute_reply.started":"2023-04-08T15:09:12.654527Z","shell.execute_reply":"2023-04-08T15:09:12.752494Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Training the Model\ntrain(model, train_dataset, test_dataset, train_steps_per_epoch=steps_per_epoch, \n      test_steps_per_epoch=test_steps, epochs=NR_EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T15:09:12.754460Z","iopub.execute_input":"2023-04-08T15:09:12.754755Z","iopub.status.idle":"2023-04-08T15:42:03.506489Z","shell.execute_reply.started":"2023-04-08T15:09:12.754708Z","shell.execute_reply":"2023-04-08T15:42:03.505574Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=4486), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4e9eda3f93a406caaf7f3cab6ff9353"}},"metadata":{}},{"name":"stdout","text":"\nTrain Step: 0, Loss: 0.8382728099822998\n\nTrain Step: 1000, Loss: 0.14716094732284546\n\nTrain Step: 2000, Loss: 0.09841518849134445\n\nTrain Step: 3000, Loss: 0.08033288270235062\n\nTrain Step: 4000, Loss: 0.07092995196580887\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Prediction and Evaluation","metadata":{}},{"cell_type":"code","source":"TEST_BATCH_SIZE = 32\ntest_steps = len(test_inputs) // TEST_BATCH_SIZE\nresult = []\n\nfor i, (token_ids, masks, labels) in enumerate(tqdm(test_dataset, total=test_steps)):\n    predictions = predict(model, token_ids, masks, labels)\n    result.append(predictions)\n\nflat_list = [item for sublist in result for item in sublist]\npredictions = []\nfor res in flat_list:\n    predictions.append(np.array(res))\n    \npredictions = pd.DataFrame(predictions, columns=target_col)\n\n# Converting the score to label using thresholding\nfor target_label in target_col:\n    predictions.loc[predictions[target_label] >= 0.5, target_label] = 1\n    predictions.loc[predictions[target_label] < 0.5, target_label] = 0\ntest_labels = pd.DataFrame(test_labels, columns=target_col)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T15:42:03.510500Z","iopub.execute_input":"2023-04-08T15:42:03.510987Z","iopub.status.idle":"2023-04-08T15:43:23.835073Z","shell.execute_reply.started":"2023-04-08T15:42:03.510934Z","shell.execute_reply":"2023-04-08T15:43:23.834226Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=498), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"404120f5d7e54b039b3c8a42148ed798"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluation_metrics(y_test, y_pred):\n    result = {}\n    result['Accuracy'] = accuracy_score(y_test, y_pred)\n    result['Precision'] = precision_score(y_test, y_pred, average='weighted')\n    result['Recall'] = recall_score(y_test, y_pred, average='weighted')\n    result['F1 Score'] = f1_score(y_test, y_pred, average='weighted')\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-04-08T15:43:23.836503Z","iopub.execute_input":"2023-04-08T15:43:23.836802Z","iopub.status.idle":"2023-04-08T15:43:23.842468Z","shell.execute_reply.started":"2023-04-08T15:43:23.836752Z","shell.execute_reply":"2023-04-08T15:43:23.841738Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"result_df = pd.DataFrame(['Target Variable','Accuracy', 'Precision', 'Recall', 'F1 Score'])\nresult_list = []\n\nfor target_label in target_col:\n    res = evaluation_metrics(test_labels[target_label],predictions[target_label])\n    res = list(res.values())\n    res.insert(0, target_label)\n    result_list.append(res) \n\npd.DataFrame(result_list, columns=['Target Variable','Accuracy', 'Precision', 'Recall', 'F1 Score'])","metadata":{"execution":{"iopub.status.busy":"2023-04-08T15:43:23.843752Z","iopub.execute_input":"2023-04-08T15:43:23.844257Z","iopub.status.idle":"2023-04-08T15:43:23.978924Z","shell.execute_reply.started":"2023-04-08T15:43:23.844188Z","shell.execute_reply":"2023-04-08T15:43:23.978171Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"  Target Variable  Accuracy  Precision    Recall  F1 Score\n0           toxic  0.838004   0.831326  0.838004  0.834632\n1    severe_toxic  0.986208   0.979383  0.986208  0.982784\n2         obscene  0.906526   0.901649  0.906526  0.904072\n3          threat  0.997116   0.994241  0.997116  0.995676\n4          insult  0.908156   0.907985  0.908156  0.908071\n5   identity_hate  0.988277   0.985150  0.988277  0.986704","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Target Variable</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>toxic</td>\n      <td>0.838004</td>\n      <td>0.831326</td>\n      <td>0.838004</td>\n      <td>0.834632</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>severe_toxic</td>\n      <td>0.986208</td>\n      <td>0.979383</td>\n      <td>0.986208</td>\n      <td>0.982784</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>obscene</td>\n      <td>0.906526</td>\n      <td>0.901649</td>\n      <td>0.906526</td>\n      <td>0.904072</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>threat</td>\n      <td>0.997116</td>\n      <td>0.994241</td>\n      <td>0.997116</td>\n      <td>0.995676</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>insult</td>\n      <td>0.908156</td>\n      <td>0.907985</td>\n      <td>0.908156</td>\n      <td>0.908071</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>identity_hate</td>\n      <td>0.988277</td>\n      <td>0.985150</td>\n      <td>0.988277</td>\n      <td>0.986704</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}